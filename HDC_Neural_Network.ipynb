{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHsyIpeobyo_",
        "outputId": "4894526e-0517-4d78-9171-7e5d45ff78ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 17.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 309kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.52MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 12.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Epoch 1/10, Loss: 17.196963158751856\n",
            "Epoch 2/10, Loss: 9.315310281476995\n",
            "Epoch 3/10, Loss: 8.038295135315039\n",
            "Epoch 4/10, Loss: 7.455595641248007\n",
            "Epoch 5/10, Loss: 7.422228387678102\n",
            "Epoch 6/10, Loss: 6.8049886272406015\n",
            "Epoch 7/10, Loss: 7.194910476456827\n",
            "Epoch 8/10, Loss: 6.645380228567225\n",
            "Epoch 9/10, Loss: 6.587520457534139\n",
            "Epoch 10/10, Loss: 5.950123872838295\n",
            "Test Accuracy: 73.63%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Hyperparameters\n",
        "DIMENSIONS = 10000  # Dimensionality of hypervectors\n",
        "NUM_CLASSES = 10    # Number of classes in Fashion MNIST\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "# Data Preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# HDC Model with Learnable Parameters\n",
        "class LearnableHDC(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, dimensions):\n",
        "        super(LearnableHDC, self).__init__()\n",
        "        self.dimensions = dimensions\n",
        "        self.num_classes = num_classes\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Learnable projection matrix\n",
        "        self.projection = nn.Parameter(torch.randn(input_size, dimensions))\n",
        "        # Learnable class hypervectors\n",
        "        self.class_vectors = nn.Parameter(torch.randn(num_classes, dimensions))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)  # Flatten images\n",
        "        proj = torch.matmul(x, self.projection)  # Project to high-dimensional space\n",
        "        proj = torch.tanh(proj)  # Non-linearity\n",
        "\n",
        "        # Compute similarity with class hypervectors\n",
        "        similarities = torch.matmul(proj, self.class_vectors.t())\n",
        "        return similarities\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = LearnableHDC(28 * 28, NUM_CLASSES, DIMENSIONS)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Testing Loop\n",
        "def test_model():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "test_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "DIMENSIONS = 10000\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 50 # Increase epochs\n",
        "LEARNING_RATE = 0.001 # Try lower learning rate\n",
        "WEIGHT_DECAY = 1e-5\n",
        "\n",
        "# Data Preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# HDC Model with Learnable Parameters\n",
        "class LearnableHDC(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, dimensions):\n",
        "        super(LearnableHDC, self).__init__()\n",
        "        self.dimensions = dimensions\n",
        "        self.num_classes = num_classes\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Using a fixed random projection (can experiment with learnable too)\n",
        "        self.projection = torch.randn(input_size, dimensions)\n",
        "        # Learnable class hypervectors\n",
        "        self.class_vectors = nn.Parameter(torch.randn(num_classes, dimensions))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)\n",
        "        # Ensure projection matrix is on the same device as input\n",
        "        self.projection = self.projection.to(x.device)\n",
        "        proj = torch.matmul(x, self.projection)\n",
        "        proj = torch.tanh(proj)\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        similarities = F.cosine_similarity(proj.unsqueeze(1), self.class_vectors.unsqueeze(0), dim=2)\n",
        "        return similarities\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LearnableHDC(28 * 28, NUM_CLASSES, DIMENSIONS).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Training Loop\n",
        "best_accuracy = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device) # Move data to GPU\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "    # Testing and saving best model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(model.state_dict(), \"best_hdc_model.pth\")\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(\"best_hdc_model.pth\"))\n",
        "print(\"Best model loaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "lFwTGmlQjDa9",
        "outputId": "dda0d010-e5e6-478f-8e26-de958fffd441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 2.2698955551139326\n",
            "Test Accuracy: 56.00%\n",
            "Epoch 2/50, Loss: 2.1827484783587425\n",
            "Test Accuracy: 64.15%\n",
            "Epoch 3/50, Loss: 2.0895802232502367\n",
            "Test Accuracy: 65.91%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4bbddc7da8ed>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Hyperparameters\n",
        "DIMENSION = 10000  # High-dimensional space\n",
        "PERTURBATION_SCALE = 0.0005  # Reduced perturbation\n",
        "LEARNING_RATE = 0.1  # Gradual reinforcement rate\n",
        "\n",
        "def random_hv():\n",
        "    return np.random.choice([-1, 1], DIMENSION)\n",
        "\n",
        "def structured_hv(value, min_val, max_val):\n",
        "    # Structured encoding based on pixel intensity\n",
        "    threshold = (value - min_val) / (max_val - min_val)\n",
        "    return np.where(np.random.rand(DIMENSION) < threshold, 1, -1)\n",
        "\n",
        "def bind(vec1, vec2):\n",
        "    return vec1 * vec2\n",
        "\n",
        "def bundle(vecs):\n",
        "    return np.sign(np.sum(vecs, axis=0))\n",
        "\n",
        "def similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2)\n",
        "\n",
        "def adaptive_redistribute(vec, feedback):\n",
        "    # Controlled redistribution based on feedback\n",
        "    perturbation = np.random.choice([-1, 1], DIMENSION) * PERTURBATION_SCALE * max(0, (1 - feedback))\n",
        "    return np.sign(vec + perturbation)\n",
        "\n",
        "def reinforce(memory_vec, input_vec, correct):\n",
        "    # Gradual reinforcement with learning rate\n",
        "    adjustment = LEARNING_RATE * (input_vec if correct else -input_vec)\n",
        "    return np.sign(memory_vec + adjustment)\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Flatten and normalize data\n",
        "X_train = X_train.reshape(-1, 28*28) / 255.0\n",
        "X_test = X_test.reshape(-1, 28*28) / 255.0\n",
        "\n",
        "# Structured feature encoding\n",
        "min_vals = X_train.min(axis=0)\n",
        "max_vals = X_train.max(axis=0)\n",
        "feature_hvs = [random_hv() for _ in range(X_train.shape[1])]\n",
        "class_hvs = {label: random_hv() for label in np.unique(y_train)}\n",
        "\n",
        "# Training phase\n",
        "memory_vectors = {label: np.zeros(DIMENSION) for label in class_hvs}\n",
        "for x, y in zip(X_train, y_train):\n",
        "    encoded_features = [bind(feature_hvs[i], structured_hv(feature, min_vals[i], max_vals[i])) for i, feature in enumerate(x)]\n",
        "    bundled_vector = bundle(encoded_features)\n",
        "    feedback = similarity(bundled_vector, memory_vectors[y]) / DIMENSION\n",
        "    bundled_vector = adaptive_redistribute(bundled_vector, feedback)\n",
        "    memory_vectors[y] = reinforce(memory_vectors[y], bundled_vector, True)\n",
        "\n",
        "# Normalize memory vectors\n",
        "for label in memory_vectors:\n",
        "    memory_vectors[label] = np.sign(memory_vectors[label])\n",
        "\n",
        "# Testing phase\n",
        "predictions = []\n",
        "for x, true_label in zip(X_test, y_test):\n",
        "    encoded_features = [bind(feature_hvs[i], structured_hv(feature, min_vals[i], max_vals[i])) for i, feature in enumerate(x)]\n",
        "    bundled_vector = bundle(encoded_features)\n",
        "    sims = {label: similarity(bundled_vector, mem_vec) for label, mem_vec in memory_vectors.items()}\n",
        "    predicted_label = max(sims, key=sims.get)\n",
        "    predictions.append(predicted_label)\n",
        "    # Gradual reinforcement for correct predictions\n",
        "    memory_vectors[true_label] = reinforce(memory_vectors[true_label], bundled_vector, predicted_label == true_label)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Model accuracy on Fashion MNIST with structured encoding and gradual reinforcement: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4HzZER-nwGJ",
        "outputId": "8b529a36-e0f0-4a5e-fcc3-a3b6d3967b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Model accuracy on Fashion MNIST with structured encoding and gradual reinforcement: 0.4140\n"
          ]
        }
      ]
    }
  ]
}